{"cells":[{"cell_type":"markdown","metadata":{"id":"_Ko9MRvIS-nA"},"source":["# Recurrent Neural Network and Multi-Head Attention (MHA)\n","\n","In this task, you will implement a conventional RNN cell, a GRU, and an MHA to understand these models. Then you will configurate GRU in special ways such that it either recovers a conventional RNN or keeps its memory in long term. NOTE: you should not change the provided function interfaces and test cases. "]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/MyDrive/cs137assignments/assignment4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3phAnY1kS_24","executionInfo":{"status":"ok","timestamp":1669435348293,"user_tz":300,"elapsed":16665,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}},"outputId":"e5b45765-7156-4090-cf08-cde1d800010e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"aQLkeC9tS-nG","executionInfo":{"status":"ok","timestamp":1669435795455,"user_tz":300,"elapsed":218,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}},"outputId":"39df8151-a6b4-4de7-cc80-acdabf5b21c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]},{"output_type":"display_data","data":{"application/javascript":["IPython.notebook.set_autosave_interval(180000)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Autosaving every 180 seconds\n"]}],"source":["# As usual, a bit of setup\n","import time\n","import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# for auto-reloading external modules\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","%autosave 180\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"]},{"cell_type":"markdown","metadata":{"id":"ToS-IGsYS-nJ"},"source":["## Recurrent Neural Networks\n","\n","In this task, you will need to implement forward calculation of recurrent neural networks. Let's first initialize a problem for RNNs."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"V5CBpbEmS-nJ","executionInfo":{"status":"ok","timestamp":1669435795818,"user_tz":300,"elapsed":2,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}}},"outputs":[],"source":["import torch.nn as nn\n","## Setup an example. Provide sizes and the input data. \n","\n","# set sizes \n","time_steps = 12\n","batch_size = 4\n","input_size = 3\n","hidden_size = 2\n","\n","# create input data with shape [batch_size, time_steps, num_features]\n","np.random.seed(137)\n","input_data = torch.randn(batch_size, time_steps, input_size, dtype = torch.float32)\n","## Create RNN layers\n","\n","# initialize a state of zero for both RNN and GRU\n","# 'state' is a tensor of shape [batch_size, hidden_size]\n","initial_state = torch.randn(batch_size, hidden_size, dtype = torch.float32).unsqueeze(0)"]},{"cell_type":"markdown","metadata":{"id":"CIG2_vidS-nK"},"source":["### Implement an RNN and a GRU with PyTorch"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"GQIBbu8US-nL","executionInfo":{"status":"ok","timestamp":1669435796171,"user_tz":300,"elapsed":2,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}}},"outputs":[],"source":["# create an RNN with only one layer from torch\n","t_rnn = nn.RNN(input_size, hidden_size, num_layers = 1, batch_first = True)\n","\n","# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n","# RNN cell outputs the hidden state directly, so the output at each step is the hidden state at that step\n","# final_state is the last state of the sequence. final_state == outputs[:, -1, :]\n","\n","# create a GRU RNN\n","t_gru = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True)\n","\n","with torch.no_grad():\n","    t_rnn_outputs, t_rnn_final_state = t_rnn(input_data, initial_state)\n","    # 'outputs' and `final_state` are the same for a GRU.\n","    t_gru_outputs, t_gru_final_state = t_gru(input_data, initial_state)"]},{"cell_type":"markdown","metadata":{"id":"9xhruxstS-nM"},"source":["### Read out parameters from RNN and GRU cells\n","\n","**Q1 (0 points)** Understanding `RNN` and `GRU` parameters\n","\n","Please read the code and documentation of `get_rnn_params` and `get_gru_params` to see how to read out parameters from these to models. You will need to use these parameters in your own implementations. NO implementation is needed here.\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"G03CdvhoS-nN","executionInfo":{"status":"ok","timestamp":1669435796561,"user_tz":300,"elapsed":2,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}}},"outputs":[],"source":["from rnn_param_helper import get_rnn_params, get_gru_params\n","\n","wt_h, wt_x, bias = get_rnn_params(t_rnn)\n","\n","# NOTE: please check the documentation of `torch.nn.GRU` and the implementation of `get_gru_params` to \n","# understand the three returning arguments.\n","\n","linear_trans_r, linear_trans_z, linear_trans_n = get_gru_params(t_gru)"]},{"cell_type":"markdown","metadata":{"id":"kvv5KDhwS-nN"},"source":["### Numpy Implementation\n","**Q2 (3 points)** Please implement your own simple RNN. \n","\n","Your implementation needs to match the tensorflow calculation.\n","\n","**Q3 (5 points)** Please implement your own GRU. \n","\n","Your implementation needs to match the tensorflow calculation."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJ3A2lvZS-nO","executionInfo":{"status":"ok","timestamp":1669435796915,"user_tz":300,"elapsed":4,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}},"outputId":"37e4f0e7-8628-4c1f-8845-3f0fc5e8eb73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between your RNN implementation and tf RNN 4.474195e-07\n","Difference between your GRU implementation and tf GRU 4.826031e-07\n"]}],"source":["from implementation import rnn,gru\n","\n","# calculation from your own implemenation of a basic RNN\n","nprnn_outputs, nprnn_final_state = rnn(wt_h, wt_x, bias, initial_state.numpy(), input_data.numpy())\n","\n","print(\"Difference between your RNN implementation and tf RNN\", \n","                     rel_error(t_rnn_outputs.numpy(), nprnn_outputs) + rel_error(t_rnn_final_state.numpy(), nprnn_final_state))\n","\n","# calculation from your own implemenation of a GRU RNN\n","npgru_outputs, npgru_final_state = gru(linear_trans_r, linear_trans_z, linear_trans_n, initial_state.numpy(), input_data.numpy())\n","\n","print(\"Difference between your GRU implementation and tf GRU\", \n","      rel_error(t_gru_outputs.numpy(), npgru_outputs) + rel_error(t_gru_final_state.numpy(), npgru_final_state))\n"]},{"cell_type":"markdown","metadata":{"id":"HynXpzssS-nP"},"source":["### GRU includes RNN as a special case\n","**Q4 (2 points)** Can you assign a special set of parameters to GRU such that its outputs is almost the same as RNN?"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IgDPIPIMS-nP","executionInfo":{"status":"ok","timestamp":1669435797433,"user_tz":300,"elapsed":177,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}},"outputId":"aa8e2dfa-7286-4ab8-dd46-a22baf84cfa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between RNN and a special GRU 4.316596e-07\n"]}],"source":["# Assign some value to a parameter of GRU\n","\n","from implementation import init_gru_with_rnn\n","\n","linear_trans_r, linear_trans_z, linear_trans_n = init_gru_with_rnn(wt_h, wt_x, bias)\n","\n","# concatenate these parameters to initialize GRU kernels\n","kernel_init = np.concatenate([linear_trans_r[0], linear_trans_z[0], linear_trans_n[0]], axis=1).T\n","rec_kernel_init = np.concatenate([linear_trans_r[2], linear_trans_z[2], linear_trans_n[2]], axis=1).T\n","bias_init0 = np.concatenate([linear_trans_r[1], linear_trans_z[1], linear_trans_n[1]], axis=0)\n","bias_init1 = np.concatenate([linear_trans_r[3], linear_trans_z[3], linear_trans_n[3]])\n","\n","grurnn = nn.GRU(input_size, hidden_size, num_layers = 1, batch_first = True)\n","wt_x1, wt_h1, bias_ih1, bias_hh1 = grurnn._flat_weights\n","\n","wt_x1.data = torch.tensor(kernel_init, dtype =torch.float32)\n","wt_h1.data = torch.tensor(rec_kernel_init, dtype = torch.float32)\n","bias_ih1.data = torch.tensor(bias_init0, dtype = torch.float32)\n","bias_hh1.data = torch.tensor(bias_init1, dtype = torch.float32)\n","\n","\n","# 'outputs' is a tensor of shape [batch_size, time_steps, hidden_size]\n","# Same as the basic RNN cell, final_state == outputs[:, -1, :]\n","with torch.no_grad():\n","    t_rnn_outputs, t_rnn_final_state = t_rnn(input_data, initial_state)\n","    grurnn_outputs, grurnn_final_state = grurnn(input_data, initial_state)\n","\n","# they are the same as the calculation from the basic RNN\n","print(\"Difference between RNN and a special GRU\", rel_error(t_rnn_outputs.numpy(), grurnn_outputs.numpy()))"]},{"cell_type":"markdown","metadata":{"id":"XUpqucxDS-nQ"},"source":["## Long-term dependency in GRUs\n"]},{"cell_type":"markdown","metadata":{"id":"9SpwmP8TS-nQ"},"source":["**Q5 (2 points)** Can you set GRU parameters such that it maintains the initial state in the memory for a long term? "]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MO8PlGHKS-nR","executionInfo":{"status":"ok","timestamp":1669435798750,"user_tz":300,"elapsed":146,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}},"outputId":"fd22aa54-728c-410f-c51a-479a8e563a71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between a later hidden state and the initial state is 0.0\n"]}],"source":["from implementation import init_gru_with_long_term_memory\n","\n","linear_trans_r, linear_trans_z, linear_trans_n = init_gru_with_long_term_memory(input_size, hidden_size)\n","\n","# concatenate these parameters to initialize GRU kernels\n","kernel_init = np.concatenate([linear_trans_r[0], linear_trans_z[0], linear_trans_n[0]], axis=1).T\n","rec_kernel_init = np.concatenate([linear_trans_r[2], linear_trans_z[2], linear_trans_n[2]], axis=1).T\n","bias_init0 = np.concatenate([linear_trans_r[1], linear_trans_z[1], linear_trans_n[1]], axis=0)\n","bias_init1 = np.concatenate([linear_trans_r[3], linear_trans_z[3], linear_trans_n[3]])\n","\n","gru2 = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n","wt_xg, wt_hg, bias_ihg, bias_hhg = gru2._flat_weights\n","\n","wt_xg.data = torch.tensor(kernel_init, dtype = torch.float32)\n","wt_hg.data = torch.tensor(rec_kernel_init, dtype = torch.float32)\n","bias_ihg.data = torch.tensor(bias_init0, dtype = torch.float32)\n","bias_hhg.data = torch.tensor(bias_init1, dtype = torch.float32)\n","\n","with torch.no_grad():\n","    outputs, _ = gru2(input_data, initial_state)\n","    outputs = outputs.numpy()\n","    print('Difference between a later hidden state and the initial state is', np.mean(np.abs(outputs[:, 10, :] - initial_state[0, :, :].numpy())))\n","    "]},{"cell_type":"markdown","source":[],"metadata":{"id":"moXlfnmUxD70"}},{"cell_type":"markdown","metadata":{"id":"hCeVAXDbS-nS"},"source":["# Implement a multi-head attention layer\n","**Q6 (5 points)** In the task, you need to implement the forward calculation of a multi-head attention layer. Your calculation needs to match the calculation of the torch MHA layer in the following test case. "]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eOsFxO1eS-nS","executionInfo":{"status":"ok","timestamp":1669435800840,"user_tz":300,"elapsed":4,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}},"outputId":"5ef1a02c-e63c-4ebf-e7fd-203b3678114e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Difference between my output and torch output is  1.7877756e-08\n"]}],"source":["from rnn_param_helper import get_mha_params\n","from implementation import mha\n","\n","\n","batch_size = 4\n","time_steps = 8\n","input_size = 10\n","num_heads = 5\n","\n","input_data = torch.randn(batch_size, time_steps, input_size, dtype = torch.float32)\n","\n","\n","# run torch implementation of MHA\n","with torch.no_grad():\n","\n","    t_mha = nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads, dropout=0.0, bias=False, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=True)\n","\n","    t_output, _ = t_mha(input_data, input_data, input_data, need_weights=False)\n","\n","\n","# extract model parameters from the torch MHA layer\n","Wq, Wk, Wv, Wo = get_mha_params(t_mha)\n","\n","# run the same calculation with your implementation\n","output = mha(Wq, Wk, Wv, Wo, input_data )\n","\n","print('Difference between my output and torch output is ', np.mean(np.abs(output - t_output.numpy())))\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"jYovP8y7S-nS","executionInfo":{"status":"ok","timestamp":1669435801897,"user_tz":300,"elapsed":125,"user":{"displayName":"Ethan Harvey","userId":"07419801998201839525"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}